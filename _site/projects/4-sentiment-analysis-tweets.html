<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#317EFB">
  
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-32x32.png">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-192x192.png" sizes="192x192">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-160x160.png" sizes="160x160">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-96x96.png" sizes="96x96">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-16x16.pn" sizes="16x16">


  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css" type="text/css"/>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css" type="text/css">

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-TNJDJLX');</script>
  <!-- End Google Tag Manager -->

  <!-- SEO Plugin -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>(4) sentiment_analysis_tweets | Wahyu Suryo Putro Bayu</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="(4) sentiment_analysis_tweets" />
<meta name="author" content="Wahyu Suryo Putro Bayu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="3-way polarity (positive, negative, neutral) classification system for tweets, without using NLTK’s sentiment analysis engine." />
<meta property="og:description" content="3-way polarity (positive, negative, neutral) classification system for tweets, without using NLTK’s sentiment analysis engine." />
<link rel="canonical" href="http://localhost:4000/projects/4-sentiment-analysis-tweets" />
<meta property="og:url" content="http://localhost:4000/projects/4-sentiment-analysis-tweets" />
<meta property="og:site_name" content="Wahyu Suryo Putro Bayu" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-21T17:00:45+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(4) sentiment_analysis_tweets" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Wahyu Suryo Putro Bayu"},"dateModified":"2022-11-21T17:00:45+07:00","datePublished":"2022-11-21T17:00:45+07:00","description":"3-way polarity (positive, negative, neutral) classification system for tweets, without using NLTK’s sentiment analysis engine.","headline":"(4) sentiment_analysis_tweets","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/4-sentiment-analysis-tweets"},"url":"http://localhost:4000/projects/4-sentiment-analysis-tweets"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body class="d-flex flex-column h-100">

  <main class="flex-shrink-0 container mt-5">
  <nav class="navbar navbar-expand-lg navbar-light">

  <a class="navbar-brand" href="/"><h5><b>Wahyu Suryo Putro Bayu</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/404.html"></a>

      <a class="nav-item nav-link active" href="/"></a>

      <a class="nav-item nav-link " href="/blog/tags"></a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/contact/">Contact Me</a>

      

    </div>
  </div>

</nav>
  <div class="notebook">
  <h1 class="notebook-title">Sentiment Analysis for Tweets</h1>
  <div class="project-skills">
  Skills:<span class="badge badge-pill text-primary border border-primary ml-1">Python</span><span class="badge badge-pill text-primary border border-primary ml-1">Scikit-learn</span><span class="badge badge-pill text-primary border border-primary ml-1">NLP</span></div>
  <a target="_blank" href=></a>
  <hr />

<h4 id="overview">Overview</h4>

<p>In this project, we’ll build a 3-way polarity (positive, negative, neutral) classification system for tweets, without using NLTK’s in-built sentiment analysis engine.</p>

<p>We’ll use a logistic regression classifier, bag-of-words features, and polarity lexicons (both in-built and external). We’ll also create our own pre-processing module to handle raw tweets.</p>

<h4 id="data-used">Data Used</h4>

<ul>
  <li>
    <p>training.json: This file contains ~15k raw tweets, along with their polarity labels (1 = positive, 0 = neutral, -1 = negative). We’ll use this file to train our classifiers.</p>
  </li>
  <li>
    <p>develop.json: In the same format as training.json, the file contains a smaller set of tweets. We’ll use it to test the predictions of our classifiers which were trained on the training set.</p>
  </li>
</ul>

<h2 id="preprocessing">Preprocessing</h2>

<p>The first thing that we’ll do is preprocess the tweets so that they’re easier to deal with, and ready for feature extraction, and training by the classifiers.</p>

<p>To start with we’re going to extract the tweets from the json file, read each line and store the tweets, labels in separate lists.</p>

<p>Then for the preprocessing, we’ll:</p>

<ul>
  <li><strong>segment</strong> tweets into sentences using an NTLK segmenter</li>
  <li><strong>tokenize</strong> the sentences using an NLTK tokenizer</li>
  <li><strong>lowercase</strong> all the words</li>
  <li><strong>remove twitter usernames</strong> beginning with @ using regex</li>
  <li><strong>remove URLs</strong> starting with http using regex</li>
  <li><strong>process hashtags</strong> ,for this we’ll tokenize hashtags,and try to break down multi-word hashtags using a MaxMatch algorithm, and the English word dictionary supplied with NLTK.</li>
</ul>

<p>Let’s build some functions to accomplish all this.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">stem</span><span class="p">.</span><span class="n">wordnet</span><span class="p">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">nltk</span><span class="p">.</span><span class="n">corpus</span><span class="p">.</span><span class="n">words</span><span class="p">.</span><span class="n">words</span><span class="p">())</span> <span class="c1">#To be used for MaxMatch
</span>
<span class="c1">#Function to lemmatize word | Used during maxmatch
</span><span class="k">def</span> <span class="nf">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">lemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="p">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="s">'v'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lemma</span> <span class="o">==</span> <span class="n">word</span><span class="p">:</span>
        <span class="n">lemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="p">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="s">'n'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lemma</span>

<span class="c1">#Function to implement the maxmatch algorithm for multi-word hashtags
</span><span class="k">def</span> <span class="nf">maxmatch</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">dictionary</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">),</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">first</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]</span>
        <span class="n">rem</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">lemmatize</span><span class="p">(</span><span class="n">first</span><span class="p">).</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span> <span class="c1">#Important to lowercase lemmatized words before comparing in dictionary. 
</span>            <span class="k">return</span> <span class="p">[</span><span class="n">first</span><span class="p">]</span> <span class="o">+</span> <span class="n">maxmatch</span><span class="p">(</span><span class="n">rem</span><span class="p">,</span><span class="n">dictionary</span><span class="p">)</span>
    <span class="n">first</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">rem</span> <span class="o">=</span> <span class="n">word</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">first</span><span class="p">]</span> <span class="o">+</span> <span class="n">maxmatch</span><span class="p">(</span><span class="n">rem</span><span class="p">,</span><span class="n">dictionary</span><span class="p">)</span>

<span class="c1">#Function to preprocess a single tweet
</span><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
    
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"@\w+"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span><span class="n">tweet</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"http\S+"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span><span class="n">tweet</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">hashtags</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="s">"#\w+"</span><span class="p">,</span><span class="n">tweet</span><span class="p">)</span>
    
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"#\w+"</span><span class="p">,</span><span class="s">""</span><span class="p">,</span><span class="n">tweet</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span> 
    
    <span class="n">hashtag_tokens</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#Separate list for hashtags
</span>    
    <span class="k">for</span> <span class="n">hashtag</span> <span class="ow">in</span> <span class="n">hashtags</span><span class="p">:</span>
        <span class="n">hashtag_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">maxmatch</span><span class="p">(</span><span class="n">hashtag</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span><span class="n">dictionary</span><span class="p">))</span>        
    
    <span class="n">segmenter</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'tokenizers/punkt/english.pickle'</span><span class="p">)</span>
    <span class="n">segmented_sentences</span> <span class="o">=</span> <span class="n">segmenter</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
    
    <span class="c1">#General tokenization
</span>    <span class="n">processed_tweet</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">word_tokenizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="n">tokenize</span><span class="p">.</span><span class="n">regexp</span><span class="p">.</span><span class="n">WordPunctTokenizer</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">segmented_sentences</span><span class="p">:</span>
        <span class="n">tokenized_sentence</span> <span class="o">=</span> <span class="n">word_tokenizer</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">.</span><span class="n">strip</span><span class="p">())</span>
        <span class="n">processed_tweet</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokenized_sentence</span><span class="p">)</span>
    
    <span class="c1">#Processing the hashtags only when they exist in a tweet
</span>    <span class="k">if</span> <span class="n">hashtag_tokens</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">tag_token</span> <span class="ow">in</span> <span class="n">hashtag_tokens</span><span class="p">:</span>
            <span class="n">processed_tweet</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag_token</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">processed_tweet</span>
    
<span class="c1">#Custom function that takes in a file, and passes each tweet to the preprocessor
</span><span class="k">def</span> <span class="nf">preprocess_file</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">tweets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">tweet_dict</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="n">tweets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">preprocess</span><span class="p">(</span><span class="n">tweet_dict</span><span class="p">[</span><span class="s">"text"</span><span class="p">]))</span>
        <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">tweet_dict</span><span class="p">[</span><span class="s">"label"</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">tweets</span><span class="p">,</span><span class="n">labels</span>
</code></pre></div></div>

<p>Before we run preprocess our training data, let’s see how well the maxmatch algorithm works.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">maxmatch</span><span class="p">(</span><span class="s">'wecan'</span><span class="p">,</span><span class="n">dictionary</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['we', 'can']
</code></pre></div></div>

<p>Let’s try feeding it something harder than that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">maxmatch</span><span class="p">(</span><span class="s">'casestudy'</span><span class="p">,</span><span class="n">dictionary</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['cases', 'tu', 'd', 'y']
</code></pre></div></div>

<p>As we can see from the above example, it incorrectly breks down the word ‘casestudy’, by returning ‘cases’, instead of ‘case’ for the first iteration., which would have been a better output. This is because it <em>greedily</em> extract ‘cases’ first.</p>

<p>For an improvement, we can implement an algorithm that better counts the total number of successful matches in the result of the maxmatch process, and return the one with the highest successful match count.</p>

<p>Let’s run our preprocessing module on the raw training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Running the basic preprocessing module and capturing the data (maybe shift to the next block)
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">preprocess_file</span><span class="p">(</span><span class="s">'data/sentiment/training.json'</span><span class="p">)</span>
<span class="n">train_tweets</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>Let’s print out the first couple processed tweets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span> <span class="n">train_tweets</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[[u'dear', u'the', u'newooffice', u'for', u'mac', u'is', u'great', u'and', u'all', u',', u'but', u'no', u'lync', u'update', u'?'], [u'c', u"'", u'mon', u'.']], [[u'how', u'about', u'you', u'make', u'a', u'system', u'that', u'doesn', u"'", u't', u'eat', u'my', u'friggin', u'discs', u'.'], [u'this', u'is', u'the', u'2nd', u'time', u'this', u'has', u'happened', u'and', u'i', u'am', u'so', u'sick', u'of', u'it', u'!']]]
</code></pre></div></div>

<p>Hmm.. we can do better than that to make sense of what’s happening. Let’s write a simple script to that’ll run the preprocessing module on a few tweets, and print the original and processed results, side by side; if it detects a multi-word hashtag.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Printing examples of multi-word hashtags (Doesn't work for multi sentence tweets)
</span><span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'data/sentiment/training.json'</span><span class="p">)</span>
<span class="n">count</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span><span class="mi">5</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">original_tweet</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)[</span><span class="s">"text"</span><span class="p">]</span>
    <span class="n">hashtags</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="s">"#\w+"</span><span class="p">,</span><span class="n">original_tweet</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hashtags</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">hashtag</span> <span class="ow">in</span> <span class="n">hashtags</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">maxmatch</span><span class="p">(</span><span class="n">hashtag</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span><span class="n">dictionary</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1">#If the length of the array returned by the maxmatch function is greater than 1,
</span>                <span class="c1">#it means that the algorithm has detected a hashtag with more than 1 word inside. 
</span>                <span class="k">print</span> <span class="nb">str</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="o">+</span> <span class="s">". Original Tweet: "</span> <span class="o">+</span> <span class="n">original_tweet</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">Processed tweet: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_tweets</span><span class="p">[</span><span class="n">index</span><span class="p">])</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Original Tweet: If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft
Processed tweet: [[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.'], [u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?'], [u'windows', u'1', u'0'], [u'x', u'box', u'one']]

2. Original Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft
Processed tweet: [[u'microsoft', u',', u'i', u'may', u'not', u'prefer', u'your', u'gaming', u'branch', u'of', u'business', u'.'], [u'but', u',', u'you', u'do', u'make', u'a', u'damn', u'fine', u'operating', u'system', u'.'], [u'Window', u's', u'1', u'0']]

3. Original Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail
Processed tweet: [[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.'], [u'Window', u's', u'1', u'0'], [u'Window', u's', u'1', u'0', u'fail']]

4. Original Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q
Processed tweet: [[u'2nd', u'computer', u'with', u'same', u'error', u'!!!'], [u'guess', u'we', u'will', u'shelve', u'this', u'until', u'sp1', u'!'], [u'Window', u's', u'1', u'0', u'fail']]

5. Original Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV
Processed tweet: [[u'sunday', u'morning', u',', u'quiet', u'day', u'so', u'time', u'to', u'welcome', u'in'], [u'Window', u's', u'1', u'0']]
</code></pre></div></div>

<p>That’s better! Our pre-processing module is working as intended.</p>

<p>The next step is to convert each processed tweet into a bag-of-words feature dictionary. We’ll allow for options to remove stopwords during the process, and also to remove <em>rare</em> words, i.e. words occuring less than n times across the whole training set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))</span>

<span class="c1">#To identify words appearing less than n times, we're creating a dictionary for the whole training set
</span>
<span class="n">total_train_bow</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">train_tweets</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">tweet</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">segment</span><span class="p">:</span>
            <span class="n">total_train_bow</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_train_bow</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1">#Function to convert pre_processed tweets to bag of words feature dictionaries
#Allows for options to remove stopwords, and also to remove words occuring less than n times in the whole training set.            
</span><span class="k">def</span> <span class="nf">convert_to_feature_dicts</span><span class="p">(</span><span class="n">tweets</span><span class="p">,</span><span class="n">remove_stop_words</span><span class="p">,</span><span class="n">n</span><span class="p">):</span> 
    <span class="n">feature_dicts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">:</span>
        <span class="c1"># build feature dictionary for tweet
</span>        <span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">remove_stop_words</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">tweet</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">segment</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span> <span class="ow">and</span> <span class="p">(</span><span class="n">n</span><span class="o">&lt;=</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">total_train_bow</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">n</span><span class="p">):</span>
                        <span class="n">feature_dict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">tweet</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">segment</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">n</span><span class="o">&lt;=</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">total_train_bow</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">n</span><span class="p">:</span>
                        <span class="n">feature_dict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">feature_dicts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_dicts</span>
</code></pre></div></div>

<p>Now that we have our function to convert raw tweets to feature dictionaries, let’s run it on our training and development data. We’ll also convert the feature dictionary to a <a href="http://scikit-learn.org/stable/modules/feature_extraction.html">sparse representation</a>, so that it can be used by scikit’s ML algorithms.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">()</span>

<span class="c1">#Conversion to feature dictionaries
</span><span class="n">train_set</span> <span class="o">=</span> <span class="n">convert_to_feature_dicts</span><span class="p">(</span><span class="n">train_tweets</span><span class="p">,</span><span class="bp">True</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">dev_data</span> <span class="o">=</span> <span class="n">preprocess_file</span><span class="p">(</span><span class="s">'data/sentiment/develop.json'</span><span class="p">)</span>

<span class="n">dev_set</span> <span class="o">=</span> <span class="n">convert_to_feature_dicts</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">False</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#Conversion to sparse representations
</span><span class="n">training_data</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>

<span class="n">development_data</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dev_set</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="classifying">Classifying</h2>

<p>Now, we’ll run our data through a decision tree classifier, and try to tune the parameters by using Grid Search over parameter combinations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1">#Grid used to test the combinations of parameters
</span><span class="n">tree_param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">'criterion'</span><span class="p">:[</span><span class="s">'gini'</span><span class="p">,</span><span class="s">'entropy'</span><span class="p">],</span> <span class="s">'min_samples_leaf'</span><span class="p">:</span> <span class="p">[</span><span class="mi">75</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">125</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">175</span><span class="p">],</span> <span class="s">'max_features'</span><span class="p">:[</span><span class="s">'sqrt'</span><span class="p">,</span><span class="s">'log2'</span><span class="p">,</span><span class="bp">None</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">tree_clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span><span class="n">tree_param_grid</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>

<span class="n">tree_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span><span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="k">print</span> <span class="s">"Optimal parameters for DT: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tree_clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span> <span class="c1">#To print out the best discovered combination of the parameters
</span>
<span class="n">tree_predictions</span> <span class="o">=</span> <span class="n">tree_clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">development_data</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"</span><span class="se">\n</span><span class="s">Decision Tree Accuracy: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">tree_predictions</span><span class="p">))</span>


</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Optimal parameters for DT: {'max_features': None, 'criterion': 'entropy', 'min_samples_leaf': 75}

Decision Tree Accuracy: 0.487151448879
</code></pre></div></div>

<p>The decision tree classifier doesn’t seem to work very well, but we still don’t have a benchmark to compare it with.</p>

<p>Let’s run our data through a dummy classifier which’ll pick the most frequently occuring class as the output, each time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>

<span class="c1">#The dummy classifier below always predicts the most frequent class, as specified in the strategy. 
</span><span class="n">dummy_clf</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s">'most_frequent'</span><span class="p">)</span>
<span class="n">dummy_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">development_data</span><span class="p">,</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">dummy_predictions</span> <span class="o">=</span> <span class="n">dummy_clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">development_data</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"</span><span class="se">\n</span><span class="s">Most common class baseline accuracy: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">dummy_predictions</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Most common class baseline accuracy: 0.420448332422
</code></pre></div></div>

<p>We can see that out DT classifier at least performs better than the dummy classifier.</p>

<p>We’ll do the same process for logisitc regression classifier now.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">log_param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">'C'</span><span class="p">:[</span><span class="mf">0.012</span><span class="p">,</span><span class="mf">0.0125</span><span class="p">,</span><span class="mf">0.130</span><span class="p">,</span><span class="mf">0.135</span><span class="p">,</span><span class="mf">0.14</span><span class="p">],</span>
     <span class="s">'solver'</span><span class="p">:[</span><span class="s">'lbfgs'</span><span class="p">],</span><span class="s">'multi_class'</span><span class="p">:[</span><span class="s">'multinomial'</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">log_clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span><span class="n">log_param_grid</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>

<span class="n">log_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span><span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">log_predictions</span> <span class="o">=</span> <span class="n">log_clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">development_data</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"Optimal parameters for LR: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">log_clf</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"Logistic Regression Accuracy: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">log_predictions</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Optimal parameters for LR: {'multi_class': 'multinomial', 'C': 0.012, 'solver': 'lbfgs'}
Logistic Regression Accuracy: 0.493165664297
</code></pre></div></div>

<p>To recap what just happened, we created a logistic regression classifier by doing a grid search for the best parameters for C (regularization parameter), solver type, and multi_class handling, just like we did for the decision tree classifier.</p>

<p>We also created a dummy classifier that just picks the most common class in the development set for each prediction.</p>

<p>The table below describes the different classifiers and their accuracy scores.</p>

<table>
<tr>
<th>Classifier</th>
<th>Approx. Accuracy score (in %)</th>
</tr>
<tr>
<td>Dummy classifier (most common class)</td>
<td>42</td>
</tr>
<tr>
<td>Decision Tree classifier</td>
<td>48.7</td>
</tr>
<tr>
<td>Logistic Regression classifier</td>
<td>49.3</td>
</tr>
</table>

<p>As we can see, both classifiers are better than the ‘dummy’ classifier which just picks the most common class all the time.</p>

<h2 id="polarity-lexicons">Polarity Lexicons</h2>

<p>Now, we’ll try to integrate external information into the training set, in the form polarity scores for the tweets.</p>

<p>We’ll build two automatic lexicons, compare it with NLTK’s manually annotated set, and then add that information to our training data.</p>

<p>The <strong>first lexicon</strong> will be built through SentiWordNet. This has pre-calculated scores positive, negative and neutral sentiments for some words in WordNet. As this information is arranged in the form of synsets, we’ll just take the most common polarity across its senses (and take neutral in case of a tie).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">sentiwordnet</span> <span class="k">as</span> <span class="n">swn</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">swn_positive</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">swn_negative</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#Function supplied with the assignment, not described below.
</span><span class="k">def</span> <span class="nf">get_polarity_type</span><span class="p">(</span><span class="n">synset_name</span><span class="p">):</span>
    <span class="n">swn_synset</span> <span class="o">=</span>  <span class="n">swn</span><span class="p">.</span><span class="n">senti_synset</span><span class="p">(</span><span class="n">synset_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">swn_synset</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">elif</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">pos_score</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">neg_score</span><span class="p">()</span> <span class="ow">and</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">pos_score</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">obj_score</span><span class="p">():</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">neg_score</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">pos_score</span><span class="p">()</span> <span class="ow">and</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">neg_score</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">swn_synset</span><span class="p">.</span><span class="n">obj_score</span><span class="p">():</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>


<span class="k">for</span> <span class="n">synset</span> <span class="ow">in</span> <span class="n">wn</span><span class="p">.</span><span class="n">all_synsets</span><span class="p">():</span>      
    
    <span class="c1"># count synset polarity for each lemma
</span>    <span class="n">pos_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neg_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neutral_count</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">lemma</span> <span class="ow">in</span> <span class="n">synset</span><span class="p">.</span><span class="n">lemma_names</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">syns</span> <span class="ow">in</span> <span class="n">wn</span><span class="p">.</span><span class="n">synsets</span><span class="p">(</span><span class="n">lemma</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">get_polarity_type</span><span class="p">(</span><span class="n">syns</span><span class="p">.</span><span class="n">name</span><span class="p">())</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">pos_count</span><span class="o">+=</span><span class="mi">1</span>
            <span class="k">elif</span> <span class="n">get_polarity_type</span><span class="p">(</span><span class="n">syns</span><span class="p">.</span><span class="n">name</span><span class="p">())</span><span class="o">==-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">neg_count</span><span class="o">+=</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">neutral_count</span><span class="o">+=</span><span class="mi">1</span>
    
    <span class="k">if</span> <span class="n">pos_count</span> <span class="o">&gt;</span> <span class="n">neg_count</span> <span class="ow">and</span> <span class="n">pos_count</span> <span class="o">&gt;=</span> <span class="n">neutral_count</span><span class="p">:</span> <span class="c1">#&gt;=neutral as words that are more positive than negative, 
</span>                                                                <span class="c1">#despite being equally neutral might belong to positive list (explain)
</span>        <span class="n">swn_positive</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">synset</span><span class="p">.</span><span class="n">lemma_names</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">neg_count</span> <span class="o">&gt;</span> <span class="n">pos_count</span> <span class="ow">and</span> <span class="n">neg_count</span> <span class="o">&gt;=</span> <span class="n">neutral_count</span><span class="p">:</span>
        <span class="n">swn_negative</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">synset</span><span class="p">.</span><span class="n">lemma_names</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>       

<span class="n">swn_positive</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">swn_positive</span><span class="p">))</span>
<span class="n">swn_negative</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">swn_negative</span><span class="p">))</span>
            
            
<span class="k">print</span> <span class="s">'Positive words: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">swn_positive</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">print</span> <span class="s">'Negative Words: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">swn_negative</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Positive words: [u'mercy', u'prudent', u'blue_ribbon', u'synergistically', u'controversial']
Negative Words: [u'gynobase', u'anger', u'unservile', u'intestate', u'paresthesia']
</code></pre></div></div>

<p>I’ll try and explain what happened.</p>

<p>To calculate the polarity of a synset across its senses, the lemma names were extracted from the synset to get its ‘senses’. Then, each of those lemma names were converted to a synset object, which was then passed to the pre-supplied ‘get_polarity_type’ function. Based on the score returned, the head lemma of the synset object was appended to the relevant list. The head lemma was chosen from the lemma_names, as it best represents the synset object.</p>

<p>As the code above returns a random sample of positive and negative words each time, the words returned when I ran the code the first time (different from the above) were:</p>

<p>Positive words: [u’counterblast’, u’unperceptiveness’, u’eater’, u’white_magic’, u’cuckoo-bumblebee’]
Negative Words: [u’sun_spurge’, u’pinkness’, u’hardness’, u’unready’, u’occlusive’]</p>

<p>At first glance, they seem like a better than average sample of negative words, and a worse than average sample of positive ones.</p>

<p>This might be due to the fact that, when looking at a word like ‘unperceptiveness’, which is a positive word prefixed to convert into a negative one, or an antonym. It’s lemmas/senses might contain more positive senses of ‘perceptiveness’ than negative ones, and has hence been classified as a positive word, which might be wrong.</p>

<p>For the <strong>second lexicon</strong>, we will use the word2vec (CBOW) vectors included in NLTK.</p>

<p>Using a small set of positive and negative seed terms, we will calculate the cosine similarity between vectors of seed terms and another word. We can use Gensim to iterate over words in model.vocab for comparison over seed terms.</p>

<p>After calculating the cosine similarity of a word with both the positive and negative terms, we’ll calculate their average, after flipping the sign for negative seeds. A threshold of ±0.03 will be used to determine if words are positive or negative.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.data</span> <span class="kn">import</span> <span class="n">find</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">positive_seeds</span> <span class="o">=</span> <span class="p">[</span><span class="s">"good"</span><span class="p">,</span><span class="s">"nice"</span><span class="p">,</span><span class="s">"excellent"</span><span class="p">,</span><span class="s">"positive"</span><span class="p">,</span><span class="s">"fortunate"</span><span class="p">,</span><span class="s">"correct"</span><span class="p">,</span><span class="s">"superior"</span><span class="p">,</span><span class="s">"great"</span><span class="p">]</span>
<span class="n">negative_seeds</span> <span class="o">=</span> <span class="p">[</span><span class="s">"bad"</span><span class="p">,</span><span class="s">"nasty"</span><span class="p">,</span><span class="s">"poor"</span><span class="p">,</span><span class="s">"negative"</span><span class="p">,</span><span class="s">"unfortunate"</span><span class="p">,</span><span class="s">"wrong"</span><span class="p">,</span><span class="s">"inferior"</span><span class="p">,</span><span class="s">"awful"</span><span class="p">]</span>

<span class="n">word2vec_sample</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">find</span><span class="p">(</span><span class="s">'models/word2vec_sample/pruned.word2vec.txt'</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Word2Vec</span><span class="p">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">word2vec_sample</span><span class="p">,</span><span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">wv_positive</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">wv_negative</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">vocab</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span>
    
        <span class="n">pos_score</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">neg_score</span> <span class="o">=</span> <span class="mf">0.0</span>
    
        <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">positive_seeds</span><span class="p">:</span>
            <span class="n">pos_score</span> <span class="o">=</span> <span class="n">pos_score</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">seed</span><span class="p">)</span>
    
        <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">negative_seeds</span><span class="p">:</span>
            <span class="n">neg_score</span> <span class="o">=</span> <span class="n">neg_score</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">seed</span><span class="p">)</span>
        
        <span class="n">avg</span> <span class="o">=</span> <span class="p">(</span><span class="n">pos_score</span> <span class="o">-</span> <span class="n">neg_score</span><span class="p">)</span><span class="o">/</span><span class="mi">16</span> <span class="c1">#Total number of seeds is 16
</span>    
        <span class="k">if</span> <span class="n">avg</span><span class="o">&gt;</span><span class="mf">0.03</span><span class="p">:</span>
            <span class="n">wv_positive</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">avg</span><span class="o">&lt;-</span><span class="mf">0.03</span><span class="p">:</span>
            <span class="n">wv_negative</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">pass</span>
    

<span class="k">print</span> <span class="s">'Positive words: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">wv_positive</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">print</span> <span class="s">'Negative Words: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">wv_negative</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Positive words: [u'hoping', u'treble', u'revolutionary', u'sumptuous', u'productive']
Negative Words: [u'lawless', u'trudged', u'perpetuation', u'mystified', u'tendency']
</code></pre></div></div>

<p>Again, the code randomises the printed positive and negative words. Tn my first instance, they were:</p>

<p>Positive words: [u’elegant’, u’demonstrated’, u’retained’, u’titles’, u’strengthen’]
Negative Words: [u’scathingly’, u’anorexia’, u’rioted’, u’blunders’, u’alters’]</p>

<p>This looks like a great set of both positive negative words, looking at the samples. But let’s see how it compares with NLTK’s manually annotated set.</p>

<p>The Hu and Liu lexicon included with NLTK, has a list of positive and negative words.</p>

<p>First, we’ll investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are not in the seed set, evaluate the accuracy of with each of the automatic lexicons.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">opinion_lexicon</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="n">positive_words</span> <span class="o">=</span> <span class="n">opinion_lexicon</span><span class="p">.</span><span class="n">positive</span><span class="p">()</span>
<span class="n">negative_words</span> <span class="o">=</span> <span class="n">opinion_lexicon</span><span class="p">.</span><span class="n">negative</span><span class="p">()</span>

<span class="c1">#Calculate the percentage of words in the manually annotated lexicon set, that also appear in an automatic lexicon.
</span><span class="k">def</span> <span class="nf">get_perc_manual</span><span class="p">(</span><span class="n">manual_pos</span><span class="p">,</span><span class="n">manual_neg</span><span class="p">,</span><span class="n">auto_pos</span><span class="p">,</span><span class="n">auto_neg</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">manual_pos</span><span class="o">+</span><span class="n">manual_neg</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">auto_pos</span><span class="o">+</span><span class="n">auto_neg</span><span class="p">)))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">manual_pos</span><span class="o">+</span><span class="n">manual_neg</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

<span class="k">print</span> <span class="s">"% of words in manual lexicons, also present in the automatic lexicon"</span>
<span class="k">print</span> <span class="s">"First automatic lexicon: "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_perc_manual</span><span class="p">(</span><span class="n">positive_words</span><span class="p">,</span><span class="n">negative_words</span><span class="p">,</span><span class="n">swn_positive</span><span class="p">,</span><span class="n">swn_negative</span><span class="p">))</span>
<span class="k">print</span> <span class="s">"Second automatic lexicon: "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_perc_manual</span><span class="p">(</span><span class="n">positive_words</span><span class="p">,</span><span class="n">negative_words</span><span class="p">,</span><span class="n">wv_positive</span><span class="p">,</span><span class="n">wv_negative</span><span class="p">))</span>

<span class="c1">#Calculate the accuracy of words in the automatic lexicon. Assuming that the manual lexicons are accurate, it calculates the percentage of words that occur in both positive and negative (respectively) lists of automatic and manual lexicons.
</span><span class="k">def</span> <span class="nf">get_lexicon_accuracy</span><span class="p">(</span><span class="n">manual_pos</span><span class="p">,</span><span class="n">manual_neg</span><span class="p">,</span><span class="n">auto_pos</span><span class="p">,</span><span class="n">auto_neg</span><span class="p">):</span>
    <span class="n">common_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">manual_pos</span><span class="o">+</span><span class="n">manual_neg</span><span class="p">).</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">auto_pos</span><span class="o">+</span><span class="n">auto_neg</span><span class="p">))</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">negative_seeds</span><span class="p">)</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">positive_seeds</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">manual_pos</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">auto_pos</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">common_words</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">manual_neg</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">auto_neg</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">common_words</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">common_words</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

<span class="k">print</span> <span class="s">"</span><span class="se">\n</span><span class="s">Accuracy of lexicons: "</span>
<span class="k">print</span> <span class="s">"First automatic lexicon: "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_lexicon_accuracy</span><span class="p">(</span><span class="n">positive_words</span><span class="p">,</span><span class="n">negative_words</span><span class="p">,</span><span class="n">swn_positive</span><span class="p">,</span><span class="n">swn_negative</span><span class="p">))</span>
<span class="k">print</span> <span class="s">"Second automatic lexicon: "</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_lexicon_accuracy</span><span class="p">(</span><span class="n">positive_words</span><span class="p">,</span><span class="n">negative_words</span><span class="p">,</span><span class="n">wv_positive</span><span class="p">,</span><span class="n">wv_negative</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>% of words in manual lexicons, also present in the automatic lexicon
First automatic lexicon: 7.42377375166
Second automatic lexicon: 37.7964354102

Accuracy of lexicons: 
First automatic lexicon: 82.4701195219
Second automatic lexicon: 98.9415915327
</code></pre></div></div>

<p>The second lexicon shares the most common words with the manual lexicon, and has the most accurately classified words, as it uses the most intutive way of creative positive/negative lexicons i.e. by identifying the most similar words.</p>

<h2 id="lexicons-for-classification">Lexicons for Classification</h2>

<p>What if we used the lexicons for the main classification problem?</p>

<p>Let’s create a function that calculates a polarity score for a sentence based on a given lexicon. We’ll count the positive and negative words that appear in the tweet, and then return a +1 if there are more posiitve words, a -1 if there are more negative words, and a 0 otherwise.</p>

<p>We’ll then compare the results of the three lexicons on the development set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#All lexicons are converted to sets for faster preprocessing.
</span><span class="n">manual_pos_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">positive_words</span><span class="p">)</span>
<span class="n">manual_neg_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">negative_words</span><span class="p">)</span>

<span class="n">syn_pos_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">swn_positive</span><span class="p">)</span>
<span class="n">syn_neg_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">swn_negative</span><span class="p">)</span>

<span class="n">wordvec_pos_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">wv_positive</span><span class="p">)</span>
<span class="n">wordvec_neg_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">wv_negative</span><span class="p">)</span>

<span class="c1">#Function to calculate the polarity score of a sentence based on the frequency of positive or negative words. 
</span><span class="k">def</span> <span class="nf">get_polarity_score</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span><span class="n">pos_lexicon</span><span class="p">,</span><span class="n">neg_lexicon</span><span class="p">):</span>
    <span class="n">pos_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neg_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">pos_lexicon</span><span class="p">:</span>
            <span class="n">pos_count</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">neg_lexicon</span><span class="p">:</span>
            <span class="n">neg_count</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">if</span> <span class="n">pos_count</span><span class="o">&gt;</span><span class="n">neg_count</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">neg_count</span><span class="o">&gt;</span><span class="n">pos_count</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    

<span class="c1">#Function to calculate the score for each tweet, and compare it against the actual labels of the dataset and calculate/count the accuracy score. 
</span><span class="k">def</span> <span class="nf">data_polarity_accuracy</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">datalabels</span><span class="p">,</span><span class="n">pos_lexicon</span><span class="p">,</span><span class="n">neg_lexicon</span><span class="p">):</span>
    <span class="n">accuracy_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">tweet</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">datalabels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">==</span><span class="n">get_polarity_score</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tweet</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">],</span><span class="n">pos_lexicon</span><span class="p">,</span><span class="n">neg_lexicon</span><span class="p">):</span>
            <span class="n">accuracy_count</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">accuracy_count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span>
        
<span class="k">print</span> <span class="s">"Manual lexicon accuracy: "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">data_polarity_accuracy</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">manual_pos_set</span><span class="p">,</span><span class="n">manual_neg_set</span><span class="p">))</span>      
<span class="k">print</span> <span class="s">"First auto lexicon accuracy: "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">data_polarity_accuracy</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">syn_pos_set</span><span class="p">,</span><span class="n">syn_neg_set</span><span class="p">))</span>      
<span class="k">print</span> <span class="s">"Second auto lexicon accuracy: "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">data_polarity_accuracy</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">wordvec_pos_set</span><span class="p">,</span><span class="n">wordvec_neg_set</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Manual lexicon accuracy: 45.2159650082
First auto lexicon accuracy: 38.9283761618
Second auto lexicon accuracy: 45.1612903226
</code></pre></div></div>

<p>As we can see, the results reflect the quality metric obtained from the previous section, with the manual and second lexicon (word vector) winning out, while still not being as good as a Machine Learning algorithm without the polarity information.</p>

<h2 id="polarity-lexicon-with-machine-learning">Polarity Lexicon with Machine Learning</h2>

<p>To conclude, we’ll investigate the effects of adding the polarity score as a feature for our statistical classifier.</p>

<p>We’ll create a new version of our feature extraction function, to integrate the extra <em>feature</em> and retrain our logisitc regression classifier to see if there’s an improvement.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convert_to_feature_dicts_v2</span><span class="p">(</span><span class="n">tweets</span><span class="p">,</span><span class="n">manual</span><span class="p">,</span><span class="n">first</span><span class="p">,</span><span class="n">second</span><span class="p">,</span><span class="n">remove_stop_words</span><span class="p">,</span><span class="n">n</span><span class="p">):</span> 
    <span class="n">feature_dicts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">:</span>
        <span class="c1"># build feature dictionary for tweet
</span>        <span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">remove_stop_words</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">tweet</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">segment</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span> <span class="ow">and</span> <span class="p">(</span><span class="n">n</span><span class="o">&lt;=</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">total_train_bow</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">n</span><span class="p">):</span>
                        <span class="n">feature_dict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">tweet</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">segment</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">n</span><span class="o">&lt;=</span><span class="mi">0</span> <span class="ow">or</span> <span class="n">total_train_bow</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">n</span><span class="p">:</span>
                        <span class="n">feature_dict</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">manual</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">feature_dict</span><span class="p">[</span><span class="s">'manual_polarity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_polarity_score</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tweet</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">],</span><span class="n">manual_pos_set</span><span class="p">,</span><span class="n">manual_neg_set</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">first</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">feature_dict</span><span class="p">[</span><span class="s">'synset_polarity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_polarity_score</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tweet</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">],</span><span class="n">syn_pos_set</span><span class="p">,</span><span class="n">syn_neg_set</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">second</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">feature_dict</span><span class="p">[</span><span class="s">'wordvec_polarity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_polarity_score</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tweet</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">],</span><span class="n">wordvec_pos_set</span><span class="p">,</span><span class="n">wordvec_neg_set</span><span class="p">)</span>
    
        <span class="n">feature_dicts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">)</span>      
    <span class="k">return</span> <span class="n">feature_dicts</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">training_set_v2</span> <span class="o">=</span> <span class="n">convert_to_feature_dicts_v2</span><span class="p">(</span><span class="n">train_tweets</span><span class="p">,</span><span class="bp">True</span><span class="p">,</span><span class="bp">False</span><span class="p">,</span><span class="bp">True</span><span class="p">,</span><span class="bp">True</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">training_data_v2</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">training_set_v2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dev_set_v2</span> <span class="o">=</span> <span class="n">convert_to_feature_dicts_v2</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">True</span><span class="p">,</span><span class="bp">False</span><span class="p">,</span><span class="bp">True</span><span class="p">,</span><span class="bp">False</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">development_data_v2</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dev_set_v2</span><span class="p">)</span>

<span class="n">log_clf_v2</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.012</span><span class="p">,</span><span class="n">solver</span><span class="o">=</span><span class="s">'lbfgs'</span><span class="p">,</span><span class="n">multi_class</span><span class="o">=</span><span class="s">'multinomial'</span><span class="p">)</span>

<span class="n">log_clf_v2</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_data_v2</span><span class="p">,</span><span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">log_predictions_v2</span> <span class="o">=</span> <span class="n">log_clf_v2</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">development_data_v2</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"Logistic Regression V2 (with polarity scores) Accuracy: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">dev_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">log_predictions_v2</span><span class="p">))</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Logistic Regression V2 (with polarity scores) Accuracy: 0.507927829415
</code></pre></div></div>

<p>Though minimal, there was some improvement indeed in the classifier by integrating the polarity data.</p>

<p>This concludes our project of building a basic 3-way polarity classifier for tweets.</p>

</div>

<hr>

  <!-- MathJax script -->
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </main>

  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Wahyu Suryo Putro Bayu</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="https://www.github.com/wahyuspb"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-2x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/wahyuspb"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-2x"></i>
    </a>

</div>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script async defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script async defer src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script async defer src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script async defer src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>

</body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TNJDJLX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

</html>
