<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#317EFB">
  
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-32x32.png">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-192x192.png" sizes="192x192">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-160x160.png" sizes="160x160">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-96x96.png" sizes="96x96">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon-16x16.pn" sizes="16x16">


  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css" type="text/css"/>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css" type="text/css">

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-TNJDJLX');</script>
  <!-- End Google Tag Manager -->

  <!-- SEO Plugin -->
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>(7) boston_housing | Wahyu Suryo Putro Bayu</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="(7) boston_housing" />
<meta name="author" content="Wahyu Suryo Putro Bayu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A model to predict the value of a given house in the Boston real estate market using various statistical analysis tools. Identified the best price that a client can sell their house utilizing machine learning." />
<meta property="og:description" content="A model to predict the value of a given house in the Boston real estate market using various statistical analysis tools. Identified the best price that a client can sell their house utilizing machine learning." />
<link rel="canonical" href="http://localhost:4000/projects/7-boston-housing" />
<meta property="og:url" content="http://localhost:4000/projects/7-boston-housing" />
<meta property="og:site_name" content="Wahyu Suryo Putro Bayu" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-21T17:00:45+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(7) boston_housing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Wahyu Suryo Putro Bayu"},"dateModified":"2022-11-21T17:00:45+07:00","datePublished":"2022-11-21T17:00:45+07:00","description":"A model to predict the value of a given house in the Boston real estate market using various statistical analysis tools. Identified the best price that a client can sell their house utilizing machine learning.","headline":"(7) boston_housing","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/7-boston-housing"},"url":"http://localhost:4000/projects/7-boston-housing"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body class="d-flex flex-column h-100">

  <main class="flex-shrink-0 container mt-5">
  <nav class="navbar navbar-expand-lg navbar-light">

  <a class="navbar-brand" href="/"><h5><b>Wahyu Suryo Putro Bayu</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/404.html"></a>

      <a class="nav-item nav-link active" href="/"></a>

      <a class="nav-item nav-link " href="/blog/tags"></a>

      <a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link active" href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/contact/">Contact Me</a>

      

    </div>
  </div>

</nav>
  <div class="notebook">
  <h1 class="notebook-title">Prediction Boston Housing Prices</h1>
  <div class="project-skills">
  Skills:<span class="badge badge-pill text-primary border border-primary ml-1">Python</span><span class="badge badge-pill text-primary border border-primary ml-1">Scikit-learn</span><span class="badge badge-pill text-primary border border-primary ml-1">Decision Tree Regression</span><span class="badge badge-pill text-primary border border-primary ml-1">Model Complexity Analysis</span></div>
  <a target="_blank" href=></a>
  <hr />

<h2 id="introduction">Introduction</h2>

<p>In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from homes in suburbs of Boston, Massachusetts. A model trained on this data that is seen as a <em>good fit</em> could then be used to make certain predictions about a home’s monetary value.</p>

<p>The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Housing">UCI Machine Learning Repository</a>. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts.</p>

<p>For the purposes of this project, the following preprocessing steps have been made to the dataset:</p>
<ul>
  <li>16 data points have an <code class="language-plaintext highlighter-rouge">'MEDV'</code> value of 50.0. These data points likely contain <strong>missing or censored values</strong> and have been removed.</li>
  <li>1 data point has an <code class="language-plaintext highlighter-rouge">'RM'</code> value of 8.78. This data point can be considered an <strong>outlier</strong> and has been removed.</li>
  <li>The features <code class="language-plaintext highlighter-rouge">'RM'</code>, <code class="language-plaintext highlighter-rouge">'LSTAT'</code>, <code class="language-plaintext highlighter-rouge">'PTRATIO'</code>, and <code class="language-plaintext highlighter-rouge">'MEDV'</code> are essential. The remaining <strong>non-relevant features</strong> have been excluded.</li>
  <li>The feature <code class="language-plaintext highlighter-rouge">'MEDV'</code> has been <strong>multiplicatively scaled</strong> to account for 35 years of market inflation.</li>
</ul>

<p>We’ll start with the reading in the data, and separating the features and prices for homes into different pandas dataframes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import libraries necessary for this project
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="c1"># Import supplementary visualizations code visuals.py
</span><span class="kn">import</span> <span class="nn">visuals</span> <span class="k">as</span> <span class="n">vs</span>

<span class="c1"># Pretty display for notebooks
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Load the Boston housing dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'housing.csv'</span><span class="p">)</span>
<span class="n">prices</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'MEDV'</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'MEDV'</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    
<span class="c1"># Success
</span><span class="k">print</span> <span class="s">"Boston housing dataset has {} data points with {} variables each."</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Boston housing dataset has 489 data points with 4 variables each.
</code></pre></div></div>

<h2 id="data-exploration">Data Exploration</h2>
<p>In this section, we will make a cursory investigation about the Boston housing data.</p>

<p>Since the main goal of this project is to construct a working model which has the capability of predicting the value of houses, we have separated the dataset into <strong>features</strong> and the <strong>target variable</strong>. The <strong>features</strong>, <code class="language-plaintext highlighter-rouge">'RM'</code>, <code class="language-plaintext highlighter-rouge">'LSTAT'</code>, and <code class="language-plaintext highlighter-rouge">'PTRATIO'</code>, give us quantitative information about each data point. The <strong>target variable</strong>, <code class="language-plaintext highlighter-rouge">'MEDV'</code>, will be the variable we seek to predict. These are stored in <code class="language-plaintext highlighter-rouge">features</code> and <code class="language-plaintext highlighter-rouge">prices</code>, respectively.</p>

<h3 id="calculating-statistics">Calculating Statistics</h3>

<p>We’ll start with calculating some descriptive statistics about the Boston housing prices.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TODO: Minimum price of the data
</span><span class="n">minimum_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># TODO: Maximum price of the data
</span><span class="n">maximum_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># TODO: Mean price of the data
</span><span class="n">mean_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># TODO: Median price of the data
</span><span class="n">median_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># TODO: Standard deviation of prices of the data
</span><span class="n">std_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>

<span class="c1"># Show the calculated statistics
</span><span class="k">print</span> <span class="s">"Statistics for Boston housing dataset:</span><span class="se">\n</span><span class="s">"</span>
<span class="k">print</span> <span class="s">"Minimum price: ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">minimum_price</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Maximum price: ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">maximum_price</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Mean price: ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mean_price</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Median price ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">median_price</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Standard deviation of prices: ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">std_price</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Statistics for Boston housing dataset:

Minimum price: $105,000.00
Maximum price: $1,024,800.00
Mean price: $454,342.94
Median price $438,900.00
Standard deviation of prices: $165,171.13
</code></pre></div></div>

<h3 id="feature-observation">Feature Observation</h3>

<p>To dive a bit deeper int our data, we are using three features from the Boston housing dataset: <code class="language-plaintext highlighter-rouge">'RM'</code>, <code class="language-plaintext highlighter-rouge">'LSTAT'</code>, and <code class="language-plaintext highlighter-rouge">'PTRATIO'</code>. For each data point (neighborhood):</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">'RM'</code> is the average number of rooms among homes in the neighborhood.</li>
  <li><code class="language-plaintext highlighter-rouge">'LSTAT'</code> is the percentage of homeowners in the neighborhood considered “lower class” (working poor).</li>
  <li><code class="language-plaintext highlighter-rouge">'PTRATIO'</code> is the ratio of students to teachers in primary and secondary schools in the neighborhood.</li>
</ul>

<p>Without building a model, let’s try to figure out if an increase in the value of a feature would lead to an <strong>increase</strong> in the value of <code class="language-plaintext highlighter-rouge">'MEDV'</code> or a <strong>decrease</strong> in the value of <code class="language-plaintext highlighter-rouge">'MEDV'</code>.</p>

<ul>
  <li><strong>‘RM’:</strong> An increase in the value of this feature will lead to an increase in the value of ‘MEDV’. This is because for you’d expect a home with a higher number of rooms to be more expensive that a home with lower number of rooms.</li>
  <li><strong>‘LSTAT’:</strong> An increase in the value of this feature will lead to a decrease in the value of ‘MEDV’. A lower class homeowner might not be able to afford expensive houses, so you’d expect them to leave in a cheaper home. A higher percentage of such people could correlate to cheaper homes in an area, and thus, a lower ‘MEDV’ value.</li>
  <li><strong>‘PTRATIO’:</strong> An increase in the value of this feature will lead to an decrease in the value of ‘MEDV’. A low student to teacher ration is typically associated with better education level of a school, as a teacher is able to focus on individual students better (than if there were more students). So, due to the presence of better quality schools, people might be willing to pay more to live in those areas, to provide their children with better education, and the prices might be higher.</li>
</ul>

<p>We can build scatterplots to see if our intuition is correct.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">features</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">var</span><span class="p">],</span><span class="n">prices</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/projects/boston_housing/output_8_0.png" alt="png" /></p>

<p><img src="/images/projects/boston_housing/output_8_1.png" alt="png" /></p>

<p><img src="/images/projects/boston_housing/output_8_2.png" alt="png" /></p>

<p>All three scatterplots above confirm our intuition.</p>

<hr />

<h2 id="developing-a-model">Developing a Model</h2>
<p>In this section of the project, we will develop the tools and techniques necessary for a model to make a prediction. Being able to make accurate evaluations of each model’s performance through the use of these tools and techniques helps to greatly reinforce the confidence in your predictions.</p>

<h3 id="implementation-define-a-performance-metric">Implementation: Define a Performance Metric</h3>
<p>For this project, we will calculate the <a href="http://stattrek.com/statistics/dictionary.aspx?definition=coefficient_of_determination"><em>coefficient of determination</em></a>, R<sup>2</sup>, to quantify our model’s performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how “good” that model is at making predictions.</p>

<p>The values for R<sup>2</sup> range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the <strong>target variable</strong>. A model with an R<sup>2</sup> of 0 is no better than a model that always predicts the <em>mean</em> of the target variable, whereas a model with an R<sup>2</sup> of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the <strong>features</strong>.</p>

<p>Let’s define a function that returns the r2 score for given true and predicted data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TODO: Import 'r2_score'
</span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="k">def</span> <span class="nf">performance_metric</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">):</span>
    <span class="s">""" Calculates and returns the performance score between 
        true and predicted values based on the metric chosen. """</span>
    
    <span class="c1"># TODO: Calculate the performance score between 'y_true' and 'y_predict'
</span>    <span class="n">score</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_predict</span><span class="p">)</span>
    
    <span class="c1"># Return the score
</span>    <span class="k">return</span> <span class="n">score</span>
</code></pre></div></div>

<h3 id="goodness-of-fit">Goodness of Fit</h3>

<p>As an aside, assume that a dataset contains five data points and a model made the following predictions for the target variable:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">True Value</th>
      <th style="text-align: center">Prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">3.0</td>
      <td style="text-align: center">2.5</td>
    </tr>
    <tr>
      <td style="text-align: center">-0.5</td>
      <td style="text-align: center">0.0</td>
    </tr>
    <tr>
      <td style="text-align: center">2.0</td>
      <td style="text-align: center">2.1</td>
    </tr>
    <tr>
      <td style="text-align: center">7.0</td>
      <td style="text-align: center">7.8</td>
    </tr>
    <tr>
      <td style="text-align: center">4.2</td>
      <td style="text-align: center">5.3</td>
    </tr>
  </tbody>
</table>

<p>Let’s use the <code class="language-plaintext highlighter-rouge">performance_metric</code> function and calculate this model’s coefficient of determination.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate the performance of this model
</span><span class="n">score</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">])</span>
<span class="k">print</span> <span class="s">"Model has a coefficient of determination, R^2, of {:.3f}."</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model has a coefficient of determination, R^2, of 0.923.
</code></pre></div></div>

<p>The above model has a coefficient of determination, R<sup>2</sup>, of 0.923. Considering that the worse score for R<sup>2</sup> is 0, and the best score is 1, a score of 0.923. This means that the model successfully captures more than 90 percent of the variation in the target variable. Though it is important to note that how good the score is, depends on the domain of the problem.</p>

<h3 id="shuffle-and-split-data">Shuffle and Split Data</h3>

<p>Now we’ll take the Boston housing dataset and split the data into training and testing subsets. Typically, the data is also shuffled into a random order when creating the training and testing subsets to remove any bias in the ordering of the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TODO: Import 'train_test_split'
</span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># TODO: Shuffle and split the data into training and testing subsets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">prices</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Success
</span><span class="k">print</span> <span class="s">"Training and testing split was successful."</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training and testing split was successful.
</code></pre></div></div>

<p>By spitting a dataset into training and testing subsets, we can train our model on the training subset, and then feed it with unseen data from the test subset to evaluate the performance of our model.</p>

<p>Training and testing on the same data doesn’t give us a genuine evaluation of the model, at it has already seen testing the data when training, and thus might not perform well in real-world scenarios where we often deal with unseen data. Related to this is the problem of “overfitting”, i.e. the model can be really accurate on the training data, but perform poorly on the training data.</p>

<hr />

<h2 id="analyzing-model-performance">Analyzing Model Performance</h2>
<p>In this third section of the project, we’ll take a look at several models’ learning and testing performances on various subsets of training data. Additionally, we’ll investigate the Decision Tree algorithm with an increasing <code class="language-plaintext highlighter-rouge">'max_depth'</code> parameter on the full training set to observe how model complexity affects performance.</p>

<h3 id="learning-curves">Learning Curves</h3>
<p>The following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. The shaded region of a learning curve denotes the uncertainty of that curve (measured as the standard deviation). The model is scored on both the training and testing sets using R<sup>2</sup>, the coefficient of determination.</p>

<p>_Note: The section uses helper functions supplied for the purposes of this project and available in the ‘visuals’ module.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Produce learning curves for varying training set sizes and maximum depths
</span><span class="n">vs</span><span class="p">.</span><span class="n">ModelLearning</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">prices</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/projects/boston_housing/output_21_0.png" alt="png" /></p>

<p>Looking at the learning curve for the model with max_depth = 3,</p>

<ul>
  <li>The score of the training curve decreases as more training points are added. This happens because with fewer training points, the model can modify its paramters to better approximate the targets; but as the number of training points increases, perfectly fitting them becomes more difficult, and the training score goes down.</li>
  <li>The score of the training curve increases as more training points are added, but there’s a slight dip after adding more than 350 training points. The score starts lower because the model has not yet learned enough to predict test points. As the model receives more training points, and hence, more information, it is better suited to predict unseen data.</li>
  <li>The training and testing curve seem to be converging to a score of 0.8. This usually happens when the model has stretched its limits of extracting information from the training data even though more training points are being added. So the score stabilizes.</li>
</ul>

<p>Therefore, having more training points might not benefit the model (with max_depth of 3).</p>

<h3 id="complexity-curves">Complexity Curves</h3>
<p>The following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves — one for training and one for validation. Similar to the <strong>learning curves</strong>, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the <code class="language-plaintext highlighter-rouge">performance_metric</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vs</span><span class="p">.</span><span class="n">ModelComplexity</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/projects/boston_housing/output_24_0.png" alt="png" /></p>

<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>

<ul>
  <li>
    <p>Model trained with max_depth of 1: The model suffers from high bias at this depth. Looking at the graph, we can see that both training and validation scores are low, and similar. We can say that the model is over-simplified and is not capturing the underlying relationships present in the data for both training and validation datasets.</p>
  </li>
  <li>
    <p>Model trained with max_depth of 10: The model suffers from high variance at this depth, and is overfitting on the training data. In the graph, we can see that the training score at this depth is almost equal to 1.0, while the validation score is lower, at around 0.7. The curves also seem to be diverging away from each other at this point.</p>
  </li>
</ul>

<p>In my opinion, a maximum depth of 3 results in a model that best generalizes to unseen data. That depth is the sweet spot for model complexity, as our model performs similar on training and validation data, while the overall score for both is still relatively high at between 0.7 to 0.8. A depth lower than that gives us poor training and validation score, while a higher depth overfits on the testing data, leading to a lower validation score.</p>

<hr />

<h2 id="evaluating-model-performance">Evaluating Model Performance</h2>
<p>In this final section of the project, we will construct a model and make a prediction on the client’s feature set using an optimized model from <code class="language-plaintext highlighter-rouge">fit_model</code>.</p>

<p>We’ll be using <strong>Grid Search</strong> and <strong>Cross Validation</strong> techniques in this section.</p>

<h3 id="grid-search">Grid Search</h3>

<p>The grid search technique is a systematic way of going through different combinations of parameter values while cross validating the results to determine the parameter combination which gives the best performance based on a scoring technique.</p>

<p>In order to optimize a learning algorithm, we can apply grid search by specifying the parameters, and the possible values of those parameters. The grid search then returns the best parameter values for our model, after fitting the supplied data. This takes out the guess-work involved in seeking out the opitimal paramter values for a classifier.</p>

<h3 id="cross-validation">Cross-Validation</h3>

<p>The k-fold crossvalidation training technique is a way of splitting the dataset into <em>k</em> partitions of equal size, and then running <em>k</em> separate learning experiments on the training data. In each of the experiments, we chose a training set of the size of k-1 partitions, train our model on that partition, and evaluate the results on the remaining test data. The results/scores for the k experiments are then averaged out.</p>

<p>This technique is benefitial when using grid search to optimize a model because it allows us to look for parameter settings that perform well for different test sets. If we had a single testing set, it’s easy to tune a model to perform well for that specific test set (and result in overfitting on the test set in this case), while cross validation allows us to generalize the results.</p>

<h3 id="fitting-a-model">Fitting a Model</h3>

<p>Now we’ll bring everything together and train a model using the <strong>decision tree algorithm</strong>. To ensure that we are producing an optimized model, we will train the model using the grid search technique to optimize the <code class="language-plaintext highlighter-rouge">'max_depth'</code> parameter for the decision tree. The <code class="language-plaintext highlighter-rouge">'max_depth'</code> parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">""" Performs grid search over the 'max_depth' parameter for a 
        decision tree regressor trained on the input data [X, y]. """</span>
    
    <span class="c1"># Create cross-validation sets from the training data
</span>    <span class="n">cv_sets</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.20</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># TODO: Create a decision tree regressor object
</span>    <span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>

    <span class="c1"># TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10
</span>    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">'max_depth'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)}</span>

    <span class="c1"># TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' 
</span>    <span class="n">scoring_fnc</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">performance_metric</span><span class="p">)</span>

    <span class="c1"># TODO: Create the grid search object
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">scoring_fnc</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">cv_sets</span><span class="p">)</span>

    <span class="c1"># Fit the grid search object to the data to compute the optimal model
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Return the optimal model after fitting the data
</span>    <span class="k">return</span> <span class="n">grid</span><span class="p">.</span><span class="n">best_estimator_</span>
</code></pre></div></div>

<h3 id="making-predictions">Making Predictions</h3>
<p>Once a model has been trained on a given set of data, it can be used to make predictions on new sets of input data. In the case of a <em>decision tree regressor</em>, the model has learned <em>what the best questions to ask about the input data are</em>, and can respond with a prediction for the <strong>target variable</strong>.</p>

<h3 id="optimal-model">Optimal Model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the training data to the model using grid search
</span><span class="n">reg</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Produce the value for 'max_depth'
</span><span class="k">print</span> <span class="s">"Parameter 'max_depth' is {} for the optimal model."</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">reg</span><span class="p">.</span><span class="n">get_params</span><span class="p">()[</span><span class="s">'max_depth'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter 'max_depth' is 4 for the optimal model.
</code></pre></div></div>

<p>**Answer: ** The optimal model has a max depth of 4, which is close enough to the previous guess of 3 earlier. Going back to the complexity graph, the model might perform better on the test set with a max depth of 4, but since the difference is minimal (which can also be due to noise), it might make sense to chose the simpler model.</p>

<h3 id="predicting-selling-prices">Predicting Selling Prices</h3>

<p>Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell. You have collected the following information from three of your clients:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Feature</th>
      <th style="text-align: center">Client 1</th>
      <th style="text-align: center">Client 2</th>
      <th style="text-align: center">Client 3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Total number of rooms in home</td>
      <td style="text-align: center">5 rooms</td>
      <td style="text-align: center">4 rooms</td>
      <td style="text-align: center">8 rooms</td>
    </tr>
    <tr>
      <td style="text-align: center">Neighborhood poverty level (as %)</td>
      <td style="text-align: center">17%</td>
      <td style="text-align: center">32%</td>
      <td style="text-align: center">3%</td>
    </tr>
    <tr>
      <td style="text-align: center">Student-teacher ratio of nearby schools</td>
      <td style="text-align: center">15-to-1</td>
      <td style="text-align: center">22-to-1</td>
      <td style="text-align: center">12-to-1</td>
    </tr>
  </tbody>
</table>

<p>Let’s see what prices our model will predict for these clients, and if they seem reasonable given the features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Produce a matrix for client data
</span><span class="n">client_data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="c1"># Client 1
</span>               <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">22</span><span class="p">],</span> <span class="c1"># Client 2
</span>               <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>  <span class="c1"># Client 3
</span>
<span class="c1"># Show predictions
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">price</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">client_data</span><span class="p">)):</span>
    <span class="k">print</span> <span class="s">"Predicted selling price for Client {}'s home: ${:,.2f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">price</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predicted selling price for Client 1's home: $401,333.33
Predicted selling price for Client 2's home: $240,947.37
Predicted selling price for Client 3's home: $893,700.00
</code></pre></div></div>

<p>The prices seem reasonable given the features of the homes. We can see that the highest priced home has the most rooms, lowest neighbourhood poverty level, and the lowest student-teacher ratio, all of which make intuitive sense as discussed in the Question 1. On the contrary, the home with the lowest number of rooms, highest neighbourhood poverty level, and highest student-teacher ratio is priced the lowest in our predictions.</p>

<p>This finishes our project. We have built a decision tree regressor that performs reasonably well given the 3 features. But should the model should be used in a real-world setting? It probably shouldn’t because:</p>

<ul>
  <li>The data collected in 1978 is not really relevant today due to rising population levels and changing population density of different areas.</li>
  <li>The features present in the data that we built our model on are not likely be sufficient to describe a home. Examples of interesting features to look at may be <em>proximity to city center</em>, or <em>neighbourhood crime rate</em>.</li>
  <li>The data collected in an urban city will not be applicable in a rural city, because the people might value different aspects of a home depending on whether they live in an urban city or a rural area. For example, a person living in a rural city might value number of rooms in a home over the proximity to the city.</li>
</ul>

</div>

<hr>

  <!-- MathJax script -->
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </main>

  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Wahyu Suryo Putro Bayu</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="https://www.github.com/wahyuspb"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-2x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/wahyuspb"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-2x"></i>
    </a>

</div>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script async defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script async defer src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script async defer src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script async defer src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>

</body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TNJDJLX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

</html>
